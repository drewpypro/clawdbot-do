# LiteLLM Proxy Configuration
# Secrets are injected via Docker Swarm secrets at /run/secrets/*
# LiteLLM reads *_FILE env vars automatically

model_list:
  # Anthropic (primary)
  - model_name: claude-opus
    litellm_params:
      model: anthropic/claude-opus-4-6
      api_key: "os.environ/ANTHROPIC_API_KEY"

  - model_name: claude-sonnet
    litellm_params:
      model: anthropic/claude-sonnet-4-20250514
      api_key: "os.environ/ANTHROPIC_API_KEY"

  # Uncomment when secrets are created:
  # - model_name: gpt-4o
  #   litellm_params:
  #     model: openai/gpt-4o
  #     api_key: "os.environ/OPENAI_API_KEY"
  # - model_name: gemini-pro
  #   litellm_params:
  #     model: gemini/gemini-2.5-pro
  #     api_key: "os.environ/GOOGLE_API_KEY"

general_settings:
  master_key: "os.environ/LITELLM_MASTER_KEY"

litellm_settings:
  drop_params: true
  set_verbose: false
